{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Xhex6lOrBI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ff60d4-5c29-4505-a32e-f5b22d9a704a"
      },
      "source": [
        "pip install hazm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hazm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/13/5a7074bc11d20dbbb46239349ac3f85f7edc148b4cf68e9b8c2f8263830c/hazm-0.7.0-py3-none-any.whl (316kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 8.1MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1; platform_system != \"Windows\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/0f/1c9b49bb49821b5856a64ea6fac8d96a619b9f291d1f06999ea98a32c89c/libwapiti-0.2.1.tar.gz (233kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 36.0MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 31.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from libwapiti>=0.2.1; platform_system != \"Windows\"->hazm) (1.15.0)\n",
            "Building wheels for collected packages: libwapiti, nltk\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=153926 sha256=c5b6d5891056b934cae7dcda9d8755a5195d5133d5f3b5215a7154ce152581c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/15/54/4510dce8bb958b1cdd2c47425cbd1e1eecc0480ac9bb1fb9ab\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-cp37-none-any.whl size=1394469 sha256=59d424a073a1c2816328c1eee228c4f24d9831f0c849382279331fc0a65702b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "Successfully built libwapiti nltk\n",
            "Installing collected packages: libwapiti, nltk, hazm\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSzU4DSay6jr"
      },
      "source": [
        "# **load and preprocess text data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72BlfmEfyYvM"
      },
      "source": [
        " **load train data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej6GzbCviYCc"
      },
      "source": [
        "#import the librarise\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "\n",
        "# tokenization and normalization train data\n",
        "\n",
        "os.chdir(r'/content/drive/MyDrive/train')\n",
        "myFiles = glob.glob('*.txt')\n",
        "\n",
        "def readfile(yourfile):\n",
        "\n",
        "    a=[]\n",
        "    for file in yourfile:\n",
        "        fo = open(\"/content/drive/MyDrive/train/\"+file, \"r\")\n",
        "        a.append(fo.read())\n",
        "    return a\n",
        "text=readfile(myFiles)\n",
        "normalizer = Normalizer()\n",
        "train=[word_tokenize(normalizer.normalize(i)) for i in text]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8q4BmhylpFO"
      },
      "source": [
        "#normalization and tokenization test data\n",
        "\n",
        "os.chdir(r'/content/drive/MyDrive/test')\n",
        "myFiles = glob.glob('*.txt')\n",
        "\n",
        "def readfile(yourfile):\n",
        "\n",
        "    a=[]\n",
        "    for file in yourfile:\n",
        "        fo = open(\"/content/drive/MyDrive/test/\"+file, \"r\")\n",
        "        a.append(fo.read())\n",
        "    return a\n",
        "text=readfile(myFiles)\n",
        "normalizer = Normalizer()\n",
        "test=[word_tokenize(normalizer.normalize(i)) for i in text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-f2jcqOzLCx"
      },
      "source": [
        "# **n_garam language model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKR_iRK9zqAK"
      },
      "source": [
        "**unigaram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV2volLmz5PA"
      },
      "source": [
        "# **for add K smoothing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17a2GlOoz4Ob"
      },
      "source": [
        "def count_words(tokenized_doc):\n",
        "    \"\"\"\n",
        "    Count the number of word appearence in the tokenized sentences\n",
        "\n",
        "    Args:\n",
        "        tokenized_doc: List of lists of strings\n",
        "\n",
        "    Returns:\n",
        "        dict that maps word (str) to the frequency (int)\n",
        "\n",
        "    in this step pass the document to the function and build dictionary that hold the count of word\n",
        "    \"\"\"\n",
        "\n",
        "    word_counts = {}\n",
        "\n",
        "\n",
        "    # Loop through each sentence\n",
        "    for doc in tokenized_doc:\n",
        "\n",
        "        # Go through each token in the sentence\n",
        "        for token in doc:\n",
        "\n",
        "            # If the token is not in the dictionary yet, set the count to 1\n",
        "            if token not in word_counts.keys():\n",
        "                word_counts[token] = 1\n",
        "\n",
        "            # If the token is already in the dictionary, increment the count by 1\n",
        "            else:\n",
        "                word_counts[token] += 1\n",
        "\n",
        "    return word_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwmtZJTzC2qn"
      },
      "source": [
        "def get_words_with_nplus_frequency(tokenized_doc, count_threshold):\n",
        "    \"\"\"\n",
        "    Find the words that appear N times or more\n",
        "\n",
        "    Args:\n",
        "        tokenized_doc: List of lists of sentences\n",
        "        count_threshold: minimum number of occurrences for a word to be in the closed vocabulary.\n",
        "\n",
        "    Returns:\n",
        "        List of words that appear N times or more\n",
        "\n",
        "    at this point we will get the dictionary of word count and get rid of some word that happen rar in our document\n",
        "    and we need some kind of tereshold for identify the limitation of data\n",
        "    \"\"\"\n",
        "\n",
        "    closed_vocab = []\n",
        "\n",
        "    # Get the word couts of the tokenized sentences\n",
        "    # Use the function that you defined earlier to count the words\n",
        "    word_counts = count_words(tokenized_doc)\n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    # for each word and its count\n",
        "    for word, cnt in word_counts.items(): # complete this line\n",
        "\n",
        "        # check that the word's count\n",
        "        # is at least as great as the minimum count\n",
        "        if cnt >= count_threshold:\n",
        "\n",
        "            # append the word to the list\n",
        "            closed_vocab.append(word)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return closed_vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3ezn_E3EhlJ"
      },
      "source": [
        "def replace_oov_words_by_unk(tokenized_doc, vocabulary, unknown_token=\"<unk>\"):\n",
        "    \"\"\"\n",
        "    Replace words not in the given vocabulary with '<unk>' token.\n",
        "\n",
        "    Args:\n",
        "        tokenized_doc: List of lists of strings\n",
        "        vocabulary: List of strings that we will use\n",
        "        unknown_token: A string representing unknown (out-of-vocabulary) words\n",
        "\n",
        "    Returns:\n",
        "        List of lists of strings, with words not in the vocabulary replaced\n",
        "\n",
        "    for having an efficient vocab need to replace some rar word with <unk> token\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    vocabulary = set(vocabulary)\n",
        "\n",
        "    # Initialize a list that will hold the sentences\n",
        "    # after less frequent words are replaced by the unknown token\n",
        "    replaced_tokenized_doc = []\n",
        "\n",
        "    # Go through each sentence\n",
        "    for doc in tokenized_doc:\n",
        "\n",
        "        # Initialize the list that will contain\n",
        "        # a single sentence with \"unknown_token\" replacements\n",
        "        replaced_doc = []\n",
        "\n",
        "        # for each token in the sentence\n",
        "        for token in doc:\n",
        "\n",
        "            # Check if the token is in the closed vocabulary\n",
        "            if token in vocabulary:\n",
        "                # If so, append the word to the replaced_sentence\n",
        "                replaced_doc.append(token)\n",
        "            else:\n",
        "                # otherwise, append the unknown token instead\n",
        "                replaced_doc.append(unknown_token)\n",
        "\n",
        "        # Append the list of tokens to the list of lists\n",
        "        replaced_tokenized_doc.append(replaced_doc)\n",
        "\n",
        "    return replaced_tokenized_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BRWO4UsFTEA"
      },
      "source": [
        "def preprocess_data(train_data, test_data, count_threshold):\n",
        "    \"\"\"\n",
        "    Preprocess data, i.e.,\n",
        "        - Find tokens that appear at least N times in the training data.\n",
        "        - Replace tokens that appear less than N times by \"<unk>\" both for training and test data.\n",
        "    Args:\n",
        "        train_data, test_data: List of lists of strings.\n",
        "        count_threshold: Words whose count is less than this are\n",
        "                      treated as unknown.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of\n",
        "        - training data with low frequent words replaced by \"<unk>\"\n",
        "        - test data with low frequent words replaced by \"<unk>\"\n",
        "        - vocabulary of words that appear n times or more in the training data\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the closed vocabulary using the train data\n",
        "    vocabulary = get_words_with_nplus_frequency(train_data,count_threshold)\n",
        "\n",
        "    # For the train data, replace less common words with \"<unk>\"\n",
        "    train_data_replaced = replace_oov_words_by_unk(train_data,vocabulary)\n",
        "\n",
        "    # For the test data, replace less common words with \"<unk>\"\n",
        "    test_data_replaced = replace_oov_words_by_unk(test_data,vocabulary)\n",
        "\n",
        "    return train_data_replaced, test_data_replaced, vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0McAlOMF7Ic"
      },
      "source": [
        "train_data_processed, test_data_processed, vocabulary = preprocess_data(train,\n",
        "                                                                        test,\n",
        "                                                                        2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ3vsZlcGdlX"
      },
      "source": [
        "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
        "    \"\"\"\n",
        "    Count all n-grams in the data\n",
        "\n",
        "    Args:\n",
        "        data: List of lists of words\n",
        "        n: number of words in a sequence\n",
        "\n",
        "    Returns:\n",
        "        A dictionary that maps a tuple of n-words to its frequency\n",
        "\n",
        "    after preprocessing the data we must build our n_gram model\n",
        "    and we can add <s> and <e> at the start and end of document espetialy when we want work with the sentences\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize dictionary of n-grams and their counts\n",
        "    n_grams = {}\n",
        "\n",
        "\n",
        "\n",
        "    # Go through each sentence in the data\n",
        "    for sentence in data: # complete this line\n",
        "\n",
        "        # prepend start token n times, and  append <e> one time\n",
        "        sentence = [start_token] * n+ sentence + [end_token]\n",
        "        # convert list to tuple\n",
        "        # So that the sequence of words can be used as\n",
        "        # a key in the dictionary\n",
        "        sentence = tuple(sentence)\n",
        "\n",
        "        # Use 'i' to indicate the start of the n-gram\n",
        "        # from index 0\n",
        "        # to the last index where the end of the n-gram\n",
        "        # is within the sentence.\n",
        "\n",
        "        m = len(sentence) if n==1 else len(sentence)-1\n",
        "        for i in range(m):\n",
        "\n",
        "            # Get the n-gram from i to i+n\n",
        "            n_gram = sentence[i:i+n]\n",
        "\n",
        "            # check if the n-gram is in the dictionary\n",
        "            if n_gram in n_grams.keys():\n",
        "\n",
        "                # Increment the count for this n-gram\n",
        "                n_grams[n_gram] += 1\n",
        "            else:\n",
        "                # Initialize this n-gram count to 1\n",
        "                n_grams[n_gram] = 1\n",
        "\n",
        "    return n_grams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuVc2x8YQQOr"
      },
      "source": [
        "uni=count_n_grams(train_data_processed,1)\n",
        "bi=count_n_grams(train_data_processed,2)\n",
        "tri=count_n_grams(train_data_processed,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxZjzpsawH7N"
      },
      "source": [
        "class estimate_probability():\n",
        "\n",
        "    def __init__(self,n_gram_counts,vocabulary,k=1):\n",
        "\n",
        "        self.n_gram_counts=n_gram_counts\n",
        "        self.k=k\n",
        "        self.vocabulary_size=len(vocabulary)\n",
        "        self.vocabulary=vocabulary\n",
        "\n",
        "    def ngram(self,word,previous_n_gram,n_plus1_gram_counts):\n",
        "\n",
        "        previous_n_gram = tuple(previous_n_gram)\n",
        "\n",
        "\n",
        "        # Set the denominator\n",
        "        # If the previous n-gram exists in the dictionary of n-gram counts,\n",
        "        # Get its count.  Otherwise set the count to zero\n",
        "        # Use the dictionary that has counts for n-grams\n",
        "        previous_n_gram_count = self.n_gram_counts[previous_n_gram] if previous_n_gram in self.n_gram_counts  else 0\n",
        "\n",
        "        # Calculate the denominator using the count of the previous n gram\n",
        "        # and apply k-smoothing\n",
        "        denominator = previous_n_gram_count + self.k * self.vocabulary_size\n",
        "\n",
        "        # Define n plus 1 gram as the previous n-gram plus the current word as a tuple\n",
        "        n_plus1_gram = previous_n_gram + (word,)\n",
        "\n",
        "        # Set the count to the count in the dictionary,\n",
        "        # otherwise 0 if not in the dictionary\n",
        "        # use the dictionary that has counts for the n-gram plus current word\n",
        "        n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts  else 0\n",
        "\n",
        "        # Define the numerator use the count of the n-gram plus current word,\n",
        "        # and apply smoothing\n",
        "        numerator = n_plus1_gram_count + self.k\n",
        "\n",
        "        # Calculate the probability as the numerator divided by denominator\n",
        "        probability = numerator / denominator\n",
        "\n",
        "        return probability\n",
        "\n",
        "    def unigram(self,word):\n",
        "\n",
        "        numerator=self.n_gram_counts[(word,)] if (word,) in self.n_gram_counts else 0\n",
        "        numerator+=self.k\n",
        "\n",
        "        denominator=self.vocabulary_size + self.k * self.vocabulary_size\n",
        "        probability = numerator / denominator\n",
        "        return probability\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nardl7RzzCH7"
      },
      "source": [
        "ob=estimate_probability(uni,vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBAvAFJRzSgX",
        "outputId": "e68d28a9-c184-48cc-e732-4ecbf4267860"
      },
      "source": [
        "ob.unigram('در')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.16820987654320987"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GATEI7U0iFL"
      },
      "source": [
        "ob.ngaram('در','ورزش',bi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rD12g1jK97M"
      },
      "source": [
        "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0,unigram=False):\n",
        "    \"\"\"\n",
        "    Estimate the probabilities of next words using the n-gram counts with k-smoothing\n",
        "\n",
        "    Args:\n",
        "        previous_n_gram: A sequence of words of length n\n",
        "        n_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "        vocabulary: List of words\n",
        "        k: positive constant, smoothing parameter\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping from next words to the probability.\n",
        "\n",
        "    here we loop over the entier word\n",
        "    \"\"\"\n",
        "    e_p=estimate_probability(n_gram_counts,vocabulary,k=1)\n",
        "    # convert list to tuple to use it as a dictionary key\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "\n",
        "    # add <e> <unk> to the vocabulary\n",
        "    # <s> is not needed since it should not appear as the next word\n",
        "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
        "    vocabulary_size = len(vocabulary)\n",
        "\n",
        "    probabilities = {}\n",
        "    for word in vocabulary:\n",
        "        if unigram==False:\n",
        "            probability = e_p.ngram(word,previous_n_gram,n_plus1_gram_counts)\n",
        "        else:\n",
        "            probability = e_p.unigram(word)\n",
        "        probabilities[word] = probability\n",
        "\n",
        "    return probabilities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kdtIgP0L6xe"
      },
      "source": [
        "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
        "\n",
        "    \"\"\"\n",
        "    now we can custruct the cout matrix for n_gram language model\n",
        "    Args:\n",
        "       n_plus1_gram_counts: smooth word\n",
        "\n",
        "\n",
        "       vocabulary:vocab of words\n",
        "\n",
        "    \"\"\"\n",
        "    # add <e> <unk> to the vocabulary\n",
        "    # <s> is omitted since it should not appear as the next word\n",
        "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
        "\n",
        "    # obtain unique n-grams\n",
        "    n_grams = []\n",
        "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
        "        n_gram = n_plus1_gram[0:-1]\n",
        "        n_grams.append(n_gram)\n",
        "    n_grams = list(set(n_grams))\n",
        "\n",
        "    # mapping from n-gram to row\n",
        "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}\n",
        "    # mapping from next word to column\n",
        "    col_index = {word:j for j, word in enumerate(vocabulary)}\n",
        "\n",
        "    nrow = len(n_grams)\n",
        "    ncol = len(vocabulary)\n",
        "    count_matrix = np.zeros((nrow, ncol))\n",
        "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
        "        n_gram = n_plus1_gram[0:-1]\n",
        "        word = n_plus1_gram[-1]\n",
        "        if word not in vocabulary:\n",
        "            continue\n",
        "        i = row_index[n_gram]\n",
        "        j = col_index[word]\n",
        "        count_matrix[i, j] = count\n",
        "\n",
        "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
        "    return count_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "NEB8LuGUO13s",
        "outputId": "5c0e46b5-8343-4879-83fb-533935b523d8"
      },
      "source": [
        "display(make_count_matrix(uni\n",
        "                          , vocabulary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>حضور</th>\n",
              "      <th>دومین</th>\n",
              "      <th>خراسان</th>\n",
              "      <th>در</th>\n",
              "      <th>لیگ</th>\n",
              "      <th>برتر</th>\n",
              "      <th>فوتسال</th>\n",
              "      <th>قطعی</th>\n",
              "      <th>شد</th>\n",
              "      <th>.</th>\n",
              "      <th>به</th>\n",
              "      <th>گزارش</th>\n",
              "      <th>دانشجویان</th>\n",
              "      <th>ایران</th>\n",
              "      <th>(</th>\n",
              "      <th>ایسنا</th>\n",
              "      <th>)</th>\n",
              "      <th>-</th>\n",
              "      <th>،</th>\n",
              "      <th>امتیاز</th>\n",
              "      <th>تیم</th>\n",
              "      <th>تهران</th>\n",
              "      <th>که</th>\n",
              "      <th>فصل</th>\n",
              "      <th>گذشته</th>\n",
              "      <th>کرد</th>\n",
              "      <th>طور</th>\n",
              "      <th>خریداری</th>\n",
              "      <th>این</th>\n",
              "      <th>با</th>\n",
              "      <th>مالی</th>\n",
              "      <th>پاداش</th>\n",
              "      <th>شده</th>\n",
              "      <th>قرار</th>\n",
              "      <th>است</th>\n",
              "      <th>نام</th>\n",
              "      <th>مسابقات</th>\n",
              "      <th>یابد</th>\n",
              "      <th>حمید</th>\n",
              "      <th>سرمربی</th>\n",
              "      <th>...</th>\n",
              "      <th>حقوقی</th>\n",
              "      <th>تعامل</th>\n",
              "      <th>رعایت</th>\n",
              "      <th>قوانین</th>\n",
              "      <th>المللی</th>\n",
              "      <th>قوانینی</th>\n",
              "      <th>دهند</th>\n",
              "      <th>ان‌ها</th>\n",
              "      <th>می‌گویند</th>\n",
              "      <th>عقب</th>\n",
              "      <th>می‌گردد</th>\n",
              "      <th>بالاخره</th>\n",
              "      <th>زحمت</th>\n",
              "      <th>بگیرند</th>\n",
              "      <th>کنند</th>\n",
              "      <th>معاون</th>\n",
              "      <th>کارها</th>\n",
              "      <th>قاطعانه</th>\n",
              "      <th>لازم</th>\n",
              "      <th>کشور</th>\n",
              "      <th>فلان</th>\n",
              "      <th>وقت</th>\n",
              "      <th>قلم</th>\n",
              "      <th>سیاه</th>\n",
              "      <th>می‌گفت</th>\n",
              "      <th>توانایی</th>\n",
              "      <th>اوردن</th>\n",
              "      <th>پول</th>\n",
              "      <th>می‌خواهد</th>\n",
              "      <th>قایق</th>\n",
              "      <th>دنیامالی</th>\n",
              "      <th>توانمندی</th>\n",
              "      <th>امکانات</th>\n",
              "      <th>رشته‌ها</th>\n",
              "      <th>برخورد</th>\n",
              "      <th>اساسی</th>\n",
              "      <th>طرح</th>\n",
              "      <th>رتبه‌های</th>\n",
              "      <th>&lt;e&gt;</th>\n",
              "      <th>&lt;unk&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>()</th>\n",
              "      <td>14.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>217.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>189.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>822.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 650 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    حضور  دومین  خراسان     در  لیگ  ...  اساسی  طرح  رتبه‌های   <e>  <unk>\n",
              "()  14.0    3.0     2.0  217.0  9.0  ...    2.0  2.0       2.0  10.0  822.0\n",
              "\n",
              "[1 rows x 650 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHaeEjZnNm9p"
      },
      "source": [
        "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
        "    count_matrix = make_count_matrix(n_plus1_gram_counts, vocabulary)\n",
        "    count_matrix += k\n",
        "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
        "    return prob_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "er5dlkh4XNjl",
        "outputId": "38ec9877-b822-450c-d83b-27bdb329bbd1"
      },
      "source": [
        "display(make_probability_matrix(bi, vocabulary, k=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>حضور</th>\n",
              "      <th>دومین</th>\n",
              "      <th>خراسان</th>\n",
              "      <th>در</th>\n",
              "      <th>لیگ</th>\n",
              "      <th>برتر</th>\n",
              "      <th>فوتسال</th>\n",
              "      <th>قطعی</th>\n",
              "      <th>شد</th>\n",
              "      <th>.</th>\n",
              "      <th>به</th>\n",
              "      <th>گزارش</th>\n",
              "      <th>دانشجویان</th>\n",
              "      <th>ایران</th>\n",
              "      <th>(</th>\n",
              "      <th>ایسنا</th>\n",
              "      <th>)</th>\n",
              "      <th>-</th>\n",
              "      <th>،</th>\n",
              "      <th>امتیاز</th>\n",
              "      <th>تیم</th>\n",
              "      <th>تهران</th>\n",
              "      <th>که</th>\n",
              "      <th>فصل</th>\n",
              "      <th>گذشته</th>\n",
              "      <th>کرد</th>\n",
              "      <th>طور</th>\n",
              "      <th>خریداری</th>\n",
              "      <th>این</th>\n",
              "      <th>با</th>\n",
              "      <th>مالی</th>\n",
              "      <th>پاداش</th>\n",
              "      <th>شده</th>\n",
              "      <th>قرار</th>\n",
              "      <th>است</th>\n",
              "      <th>نام</th>\n",
              "      <th>مسابقات</th>\n",
              "      <th>یابد</th>\n",
              "      <th>حمید</th>\n",
              "      <th>سرمربی</th>\n",
              "      <th>...</th>\n",
              "      <th>حقوقی</th>\n",
              "      <th>تعامل</th>\n",
              "      <th>رعایت</th>\n",
              "      <th>قوانین</th>\n",
              "      <th>المللی</th>\n",
              "      <th>قوانینی</th>\n",
              "      <th>دهند</th>\n",
              "      <th>ان‌ها</th>\n",
              "      <th>می‌گویند</th>\n",
              "      <th>عقب</th>\n",
              "      <th>می‌گردد</th>\n",
              "      <th>بالاخره</th>\n",
              "      <th>زحمت</th>\n",
              "      <th>بگیرند</th>\n",
              "      <th>کنند</th>\n",
              "      <th>معاون</th>\n",
              "      <th>کارها</th>\n",
              "      <th>قاطعانه</th>\n",
              "      <th>لازم</th>\n",
              "      <th>کشور</th>\n",
              "      <th>فلان</th>\n",
              "      <th>وقت</th>\n",
              "      <th>قلم</th>\n",
              "      <th>سیاه</th>\n",
              "      <th>می‌گفت</th>\n",
              "      <th>توانایی</th>\n",
              "      <th>اوردن</th>\n",
              "      <th>پول</th>\n",
              "      <th>می‌خواهد</th>\n",
              "      <th>قایق</th>\n",
              "      <th>دنیامالی</th>\n",
              "      <th>توانمندی</th>\n",
              "      <th>امکانات</th>\n",
              "      <th>رشته‌ها</th>\n",
              "      <th>برخورد</th>\n",
              "      <th>اساسی</th>\n",
              "      <th>طرح</th>\n",
              "      <th>رتبه‌های</th>\n",
              "      <th>&lt;e&gt;</th>\n",
              "      <th>&lt;unk&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>(برای,)</th>\n",
              "      <td>0.005839</td>\n",
              "      <td>0.002920</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.004380</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.002920</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.004380</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.004380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(),)</th>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.003063</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.003063</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(فنی,)</th>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(باشد,)</th>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.003067</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(لازم,)</th>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.001522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(مدال‌های,)</th>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.006116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(بالایی,)</th>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(ثمره,)</th>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(روسای,)</th>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.001531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(قهرمان,)</th>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.003049</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.003049</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>650 rows × 650 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 حضور     دومین    خراسان  ...  رتبه‌های       <e>     <unk>\n",
              "(برای,)      0.005839  0.002920  0.001460  ...  0.001460  0.001460  0.004380\n",
              "(),)         0.001531  0.001531  0.001531  ...  0.001531  0.001531  0.001531\n",
              "(فنی,)       0.001534  0.001534  0.001534  ...  0.001534  0.001534  0.001534\n",
              "(باشد,)      0.001534  0.001534  0.001534  ...  0.001534  0.001534  0.001534\n",
              "(لازم,)      0.001522  0.001522  0.001522  ...  0.001522  0.001522  0.001522\n",
              "...               ...       ...       ...  ...       ...       ...       ...\n",
              "(مدال‌های,)  0.001529  0.001529  0.001529  ...  0.001529  0.001529  0.006116\n",
              "(بالایی,)    0.001531  0.001531  0.001531  ...  0.001531  0.001531  0.001531\n",
              "(ثمره,)      0.001534  0.001534  0.001534  ...  0.001534  0.001534  0.001534\n",
              "(روسای,)     0.001531  0.001531  0.001531  ...  0.001531  0.001531  0.001531\n",
              "(قهرمان,)    0.001524  0.001524  0.001524  ...  0.001524  0.001524  0.003049\n",
              "\n",
              "[650 rows x 650 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGiMpuVuN0_j"
      },
      "source": [
        "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0,unigram=False):\n",
        "    \"\"\"\n",
        "    Calculate perplexity for a list of sentences\n",
        "\n",
        "    Args:\n",
        "        sentence: List of strings\n",
        "        n_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "        vocabulary_size: number of unique words in the vocabulary\n",
        "        k: Positive smoothing constant\n",
        "\n",
        "    Returns:\n",
        "        Perplexity score\n",
        "    \"\"\"\n",
        "    # length of previous words\n",
        "    n = len(list(n_gram_counts.keys())[0])\n",
        "\n",
        "    # prepend <s> and append <e>\n",
        "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
        "\n",
        "    # Cast the sentence from a list to a tuple\n",
        "    sentence = tuple(sentence)\n",
        "\n",
        "    # length of sentence (after adding <s> and <e> tokens)\n",
        "    N = len(sentence)\n",
        "\n",
        "    # The variable p will hold the product\n",
        "    # that is calculated inside the n-root\n",
        "    # Update this in the code below\n",
        "    product_pi = 1.0\n",
        "\n",
        "\n",
        "    # Index t ranges from n to N - 1\n",
        "    for t in range(n, N):\n",
        "\n",
        "        # get the n-gram preceding the word at position t\n",
        "        n_gram = sentence[t-n:t]\n",
        "\n",
        "        # get the word at position t\n",
        "        word = sentence[t]\n",
        "\n",
        "        # Estimate the probability of the word given the n-gram\n",
        "        # using the n-gram counts, n-plus1-gram counts,\n",
        "        # vocabulary size, and smoothing constant\n",
        "        probability = estimate_probability(word,n_gram, n_gram_counts, n_plus1_gram_counts, len(vocabulary_size), k=1,unigram=False)\n",
        "\n",
        "        # Update the product of the probabilities\n",
        "        # This 'product_pi' is a cumulative product\n",
        "        # of the (1/P) factors that are calculated in the loop\n",
        "        product_pi *= 1 / probability\n",
        "\n",
        "    # Take the Nth root of the product\n",
        "    perplexity = product_pi**(1/float(N))\n",
        "\n",
        "\n",
        "    return perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUrMtuB9YBXe"
      },
      "source": [
        "#calculate_perplexity for bigram and k smoothing\n",
        "\n",
        "for doc in test_data_processed:\n",
        "    print(calculate_perplexity(doc,uni,bi,vocabulary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8Vndd3H9UFs"
      },
      "source": [
        "#calculate_perplexity for trigram and k smoothing\n",
        "\n",
        "for doc in test_data_processed:\n",
        "    print(calculate_perplexity(doc,bi,tri,vocabulary,1))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}